"""
å®Œæ•´çš„æ–‡ä»¶å¤„ç†æµç¨‹è„šæœ¬
ä»æ–‡ä»¶ä¸‹è½½åˆ°æ•°æ®ä¿å­˜çš„å®Œæ•´æµç¨‹
"""
import argparse
import sys
import time
import os
import json
from typing import Dict, Any, List
from loguru import logger
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.core.database import db_manager
from src.services.file_operations import file_ops
from src.services.document_converter import doc_converter
from src.services.document_chunker import doc_chunker
from src.services.vectorizer import vectorizer
from src.tasks.tasks import (
    query_files, download_file, convert_document, 
    chunk_document, vectorize_chunks, save_data,
    process_single_file
)
from src.core.error_handler import error_handler
from src.utils.logger_config import app_logger

class CompleteDocumentFlow:
    """å®Œæ•´çš„æ–‡æ¡£å¤„ç†æµç¨‹ç±»"""
    
    def __init__(self):
        self.app_logger = app_logger
        self.processed_files = []
        self.failed_files = []
    
    def run_complete_flow(self, query_params: Dict[str, Any] = None, 
                         use_semantic: bool = False,
                         max_files: int = None) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„æ–‡æ¡£å¤„ç†æµç¨‹
        
        Args:
            query_params: æŸ¥è¯¢å‚æ•°
            use_semantic: æ˜¯å¦ä½¿ç”¨è¯­ä¹‰åˆ‡ç‰‡
            max_files: æœ€å¤§å¤„ç†æ–‡ä»¶æ•°é‡
            
        Returns:
            å¤„ç†ç»“æœç»Ÿè®¡
        """
        start_time = time.time()
        self.app_logger.info("=" * 60)
        self.app_logger.info("å¼€å§‹æ‰§è¡Œå®Œæ•´çš„æ–‡æ¡£å¤„ç†æµç¨‹")
        self.app_logger.info("=" * 60)
        
        try:
            # æ­¥éª¤1: æŸ¥è¯¢æ–‡ä»¶åˆ—è¡¨
            self.app_logger.info("æ­¥éª¤1: æŸ¥è¯¢æ–‡ä»¶åˆ—è¡¨")
            files = self._query_files(query_params)
            if not files:
                return {
                    'success': False,
                    'error': 'æ²¡æœ‰æŸ¥è¯¢åˆ°ä»»ä½•æ–‡ä»¶',
                    'processed_files': 0,
                    'failed_files': 0,
                    'total_time': 0
                }
            
            # é™åˆ¶å¤„ç†æ–‡ä»¶æ•°é‡
            if max_files and len(files) > max_files:
                files = files[:max_files]
                self.app_logger.info(f"é™åˆ¶å¤„ç†æ–‡ä»¶æ•°é‡ä¸º: {max_files}")
            
            # æ­¥éª¤2: å¤„ç†æ¯ä¸ªæ–‡ä»¶
            self.app_logger.info(f"æ­¥éª¤2: å¼€å§‹å¤„ç† {len(files)} ä¸ªæ–‡ä»¶")
            for i, file_info in enumerate(files, 1):
                file_id = file_info.get('id') or file_info.get('file_id')
                file_name = file_info.get('name') or file_info.get('file_name')
                
                self.app_logger.info(f"å¤„ç†æ–‡ä»¶ {i}/{len(files)}: {file_name} (ID: {file_id})")
                
                try:
                    result = self._process_single_file_complete(file_id, use_semantic)
                    if result['success']:
                        self.processed_files.append({
                            'file_id': file_id,
                            'file_name': file_name,
                            'result': result
                        })
                        self.app_logger.info(f"âœ… æ–‡ä»¶å¤„ç†æˆåŠŸ: {file_name}")
                    else:
                        self.failed_files.append({
                            'file_id': file_id,
                            'file_name': file_name,
                            'error': result.get('error', 'æœªçŸ¥é”™è¯¯')
                        })
                        self.app_logger.error(f"âŒ æ–‡ä»¶å¤„ç†å¤±è´¥: {file_name} - {result.get('error')}")
                        
                except Exception as e:
                    self.failed_files.append({
                        'file_id': file_id,
                        'file_name': file_name,
                        'error': str(e)
                    })
                    self.app_logger.error(f"âŒ æ–‡ä»¶å¤„ç†å¼‚å¸¸: {file_name} - {str(e)}")
            
            # è®¡ç®—æ€»è€—æ—¶
            total_time = time.time() - start_time
            
            # ç”Ÿæˆå¤„ç†æŠ¥å‘Š
            report = self._generate_report(total_time)
            
            self.app_logger.info("=" * 60)
            self.app_logger.info("å®Œæ•´æµç¨‹æ‰§è¡Œå®Œæˆ")
            self.app_logger.info("=" * 60)
            
            return report
            
        except Exception as e:
            error_info = error_handler.handle_system_error(e, "run_complete_flow")
            self.app_logger.error(f"å®Œæ•´æµç¨‹æ‰§è¡Œå¤±è´¥: {str(e)}")
            return {
                'success': False,
                'error': str(e),
                'error_info': error_info,
                'processed_files': len(self.processed_files),
                'failed_files': len(self.failed_files),
                'total_time': time.time() - start_time
            }
    
    def _query_files(self, query_params: Dict[str, Any] = None) -> List[Dict[str, Any]]:
        """æŸ¥è¯¢æ–‡ä»¶åˆ—è¡¨"""
        try:
            self.app_logger.info("æ­£åœ¨æŸ¥è¯¢æ–‡ä»¶åˆ—è¡¨...")
            files = file_ops.query_files(query_params)
            self.app_logger.info(f"æŸ¥è¯¢åˆ° {len(files)} ä¸ªæ–‡ä»¶")
            return files
        except Exception as e:
            self.app_logger.error(f"æŸ¥è¯¢æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}")
            raise
    
    def _process_single_file_complete(self, file_id: str, use_semantic: bool = False) -> Dict[str, Any]:
        """
        å¤„ç†å•ä¸ªæ–‡ä»¶çš„å®Œæ•´æµç¨‹ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰
        
        Args:
            file_id: æ–‡ä»¶ID
            use_semantic: æ˜¯å¦ä½¿ç”¨è¯­ä¹‰åˆ‡ç‰‡
            
        Returns:
            å¤„ç†ç»“æœ
        """
        try:
            self.app_logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶: {file_id}")
            
            # è·å–æ–‡ä»¶çŠ¶æ€
            file_status = db_manager.get_file_status(file_id)
            if not file_status:
                raise Exception(f"æ–‡ä»¶çŠ¶æ€ä¸å­˜åœ¨: {file_id}")
            
            # æ­¥éª¤1: ä¸‹è½½æ–‡ä»¶
            self.app_logger.info(f"  ğŸ“¥ ä¸‹è½½æ–‡ä»¶: {file_id}")
            download_path = self._download_file_sync(file_id)
            
            # æ­¥éª¤2: è½¬æ¢æ–‡æ¡£
            self.app_logger.info(f"  ğŸ”„ è½¬æ¢æ–‡æ¡£: {file_id}")
            markdown_path = self._convert_document_sync(file_id, download_path)
            
            # æ­¥éª¤3: åˆ‡ç‰‡æ–‡æ¡£
            self.app_logger.info(f"  âœ‚ï¸  åˆ‡ç‰‡æ–‡æ¡£: {file_id}")
            chunks = self._chunk_document_sync(file_id, markdown_path, use_semantic)
            
            # æ­¥éª¤4: å‘é‡åŒ–åˆ‡ç‰‡
            self.app_logger.info(f"  ğŸ§  å‘é‡åŒ–åˆ‡ç‰‡: {file_id}")
            vectorized_chunks = self._vectorize_chunks_sync(file_id, chunks)
            
            # æ­¥éª¤5: ä¿å­˜æ•°æ®
            self.app_logger.info(f"  ğŸ’¾ ä¿å­˜æ•°æ®: {file_id}")
            save_success = self._save_data_sync(file_id, vectorized_chunks)
            
            if save_success:
                self.app_logger.info(f"  âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: {file_id}")
                return {
                    'success': True,
                    'file_id': file_id,
                    'download_path': download_path,
                    'markdown_path': markdown_path,
                    'chunk_count': len(chunks),
                    'vector_count': len(vectorized_chunks)
                }
            else:
                raise Exception("æ•°æ®ä¿å­˜å¤±è´¥")
                
        except Exception as e:
            self.app_logger.error(f"æ–‡ä»¶å¤„ç†å¤±è´¥: {file_id} - {str(e)}")
            return {
                'success': False,
                'file_id': file_id,
                'error': str(e)
            }
    
    def _download_file_sync(self, file_id: str) -> str:
        """åŒæ­¥ä¸‹è½½æ–‡ä»¶"""
        try:
            file_status = db_manager.get_file_status(file_id)
            if not file_status:
                raise Exception(f"æ–‡ä»¶çŠ¶æ€ä¸å­˜åœ¨: {file_id}")
            
            # æ›´æ–°çŠ¶æ€ä¸ºä¸‹è½½ä¸­
            db_manager.update_status(file_id, "downloading")
            
            # æ‰§è¡Œä¸‹è½½
            download_path = file_ops.download_file(
                file_id, 
                file_status.file_url, 
                file_status.file_name
            )
            
            # æ›´æ–°çŠ¶æ€ä¸ºå·²ä¸‹è½½
            db_manager.update_status(file_id, "downloaded", download_path=download_path)
            
            return download_path
            
        except Exception as e:
            db_manager.update_status(file_id, "failed", error_message=str(e))
            raise
    
    def _convert_document_sync(self, file_id: str, file_path: str) -> str:
        """åŒæ­¥è½¬æ¢æ–‡æ¡£"""
        try:
            # æ›´æ–°çŠ¶æ€ä¸ºè½¬æ¢ä¸­
            db_manager.update_status(file_id, "converting")
            
            # æ‰§è¡Œè½¬æ¢
            markdown_path = doc_converter.convert_to_markdown(file_path, file_id)
            
            # æ›´æ–°çŠ¶æ€ä¸ºå·²è½¬æ¢
            db_manager.update_status(file_id, "converted", markdown_path=markdown_path)
            
            return markdown_path
            
        except Exception as e:
            db_manager.update_status(file_id, "failed", error_message=str(e))
            raise
    
    def _chunk_document_sync(self, file_id: str, markdown_path: str, use_semantic: bool = False) -> List[Dict[str, Any]]:
        """åŒæ­¥åˆ‡ç‰‡æ–‡æ¡£"""
        try:
            # æ›´æ–°çŠ¶æ€ä¸ºåˆ‡ç‰‡ä¸­
            db_manager.update_status(file_id, "chunking")
            
            # æ‰§è¡Œåˆ‡ç‰‡
            chunks = doc_chunker.chunk_markdown(markdown_path, file_id, use_semantic)
            
            # æ›´æ–°çŠ¶æ€ä¸ºå·²åˆ‡ç‰‡
            db_manager.update_status(file_id, "chunked", chunk_count=len(chunks))
            
            return chunks
            
        except Exception as e:
            db_manager.update_status(file_id, "failed", error_message=str(e))
            raise
    
    def _vectorize_chunks_sync(self, file_id: str, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """åŒæ­¥å‘é‡åŒ–åˆ‡ç‰‡"""
        try:
            # æ›´æ–°çŠ¶æ€ä¸ºå‘é‡åŒ–ä¸­
            db_manager.update_status(file_id, "vectorizing")
            
            # æ‰§è¡Œå‘é‡åŒ–
            vectorized_chunks = vectorizer.vectorize_chunks(chunks, file_id)
            
            # æ›´æ–°çŠ¶æ€ä¸ºå·²å‘é‡åŒ–
            db_manager.update_status(file_id, "vectorized", vector_count=len(vectorized_chunks))
            
            return vectorized_chunks
            
        except Exception as e:
            db_manager.update_status(file_id, "failed", error_message=str(e))
            raise
    
    def _save_data_sync(self, file_id: str, vectorized_chunks: List[Dict[str, Any]]) -> bool:
        """åŒæ­¥ä¿å­˜æ•°æ®"""
        try:
            # æ›´æ–°çŠ¶æ€ä¸ºä¿å­˜ä¸­
            db_manager.update_status(file_id, "saving")
            
            # æ‰§è¡Œä¿å­˜
            success = vectorizer.save_data(vectorized_chunks, file_id)
            
            if success:
                # æ›´æ–°çŠ¶æ€ä¸ºå·²å®Œæˆ
                db_manager.update_status(file_id, "completed")
                return True
            else:
                raise Exception("æ•°æ®ä¿å­˜å¤±è´¥")
                
        except Exception as e:
            db_manager.update_status(file_id, "failed", error_message=str(e))
            raise
    
    def _generate_report(self, total_time: float) -> Dict[str, Any]:
        """ç”Ÿæˆå¤„ç†æŠ¥å‘Š"""
        report = {
            'success': True,
            'total_time': total_time,
            'processed_files': len(self.processed_files),
            'failed_files': len(self.failed_files),
            'success_rate': len(self.processed_files) / (len(self.processed_files) + len(self.failed_files)) * 100 if (len(self.processed_files) + len(self.failed_files)) > 0 else 0,
            'processed_file_details': self.processed_files,
            'failed_file_details': self.failed_files,
            'statistics': {
                'total_chunks': sum(f.get('result', {}).get('chunk_count', 0) for f in self.processed_files),
                'total_vectors': sum(f.get('result', {}).get('vector_count', 0) for f in self.processed_files),
                'average_processing_time': total_time / len(self.processed_files) if self.processed_files else 0
            }
        }
        
        # æ‰“å°æŠ¥å‘Š
        self.app_logger.info("ğŸ“Š å¤„ç†æŠ¥å‘Š:")
        self.app_logger.info(f"  æ€»è€—æ—¶: {total_time:.2f} ç§’")
        self.app_logger.info(f"  æˆåŠŸå¤„ç†: {len(self.processed_files)} ä¸ªæ–‡ä»¶")
        self.app_logger.info(f"  å¤„ç†å¤±è´¥: {len(self.failed_files)} ä¸ªæ–‡ä»¶")
        self.app_logger.info(f"  æˆåŠŸç‡: {report['success_rate']:.1f}%")
        self.app_logger.info(f"  æ€»åˆ‡ç‰‡æ•°: {report['statistics']['total_chunks']}")
        self.app_logger.info(f"  æ€»å‘é‡æ•°: {report['statistics']['total_vectors']}")
        
        if self.failed_files:
            self.app_logger.info("âŒ å¤±è´¥æ–‡ä»¶åˆ—è¡¨:")
            for failed_file in self.failed_files:
                self.app_logger.info(f"  - {failed_file['file_name']}: {failed_file['error']}")
        
        return report
    
    def get_processing_status(self) -> Dict[str, Any]:
        """è·å–å½“å‰å¤„ç†çŠ¶æ€"""
        try:
            stats = db_manager.get_statistics()
            return {
                'success': True,
                'statistics': stats
            }
        except Exception as e:
            return {
                'success': False,
                'error': str(e)
            }

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å®Œæ•´çš„æ–‡æ¡£å¤„ç†æµç¨‹')
    parser.add_argument('--query-params', help='æŸ¥è¯¢å‚æ•°ï¼ˆJSONæ ¼å¼ï¼‰')
    parser.add_argument('--use-semantic', action='store_true', help='ä½¿ç”¨è¯­ä¹‰åˆ‡ç‰‡')
    parser.add_argument('--max-files', type=int, help='æœ€å¤§å¤„ç†æ–‡ä»¶æ•°é‡')
    parser.add_argument('--status', action='store_true', help='åªæ˜¾ç¤ºå¤„ç†çŠ¶æ€')
    
    args = parser.parse_args()
    
    # åˆ›å»ºæµç¨‹å¤„ç†å™¨å®ä¾‹
    processor = CompleteDocumentFlow()
    
    try:
        if args.status:
            # åªæ˜¾ç¤ºçŠ¶æ€
            result = processor.get_processing_status()
            print(json.dumps(result, indent=2, ensure_ascii=False))
        else:
            # è¿è¡Œå®Œæ•´æµç¨‹
            query_params = None
            if args.query_params:
                query_params = json.loads(args.query_params)
            
            result = processor.run_complete_flow(
                query_params=query_params,
                use_semantic=args.use_semantic,
                max_files=args.max_files
            )
            
            print(json.dumps(result, indent=2, ensure_ascii=False))
        
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­æ“ä½œ")
        sys.exit(0)
    except Exception as e:
        print(f"æ‰§è¡Œå¤±è´¥: {str(e)}")
        sys.exit(1)
    finally:
        # å…³é—­æ•°æ®åº“è¿æ¥
        db_manager.close()

if __name__ == '__main__':
    main()
